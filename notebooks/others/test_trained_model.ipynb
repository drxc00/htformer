{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the trained prototype model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Go up to project root (from inside training/)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '../..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from core.utils import process_sample\n",
    "\n",
    "# Old Model\n",
    "from core.models.hierarchical_transformer_prototype import HierarchicalTransformer as HierarchicalTransformerPrototype\n",
    "# Final Model\n",
    "from core.models.hierarchical_transformer import HierarchicalTransformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a simple pipeline\n",
    "This will include:\n",
    "- using raw video data.\n",
    "- Processing the raw video data.\n",
    "- Passing the processed video data into the model.\n",
    "- Displaying the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from core.keypoint_extractor import KeypointExtractorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_extract(path: str) -> np.ndarray:\n",
    "    extractor = KeypointExtractorV2(r\"../../models/mediapipe/pose_landmarker_full.task\")\n",
    "    keypoints = extractor.extract(path)\n",
    "    \n",
    "    max_frames = 201\n",
    "    pad_len = max_frames - len(keypoints)\n",
    "    if pad_len > 0:\n",
    "        pad = np.zeros((pad_len, keypoints.shape[1], keypoints.shape[2]))  # Preserve all dimensions\n",
    "        padded_sample = np.concatenate((keypoints, pad), axis=0)\n",
    "    else:\n",
    "        padded_sample = keypoints\n",
    "\n",
    "    return np.array(padded_sample)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_extract_for_inference(video_path: str, max_frames: int = 200) -> tuple[np.ndarray, np.ndarray]:\n",
    "    extractor = KeypointExtractorV2(r\"../../models/mediapipe/pose_landmarker_full.task\")\n",
    "    keypoints = extractor.extract(video_path) # keypoints will be (actual_len, num_keypoints, coords)\n",
    "    padded_sample, attention_mask,_ = process_sample(keypoints, max_frames)\n",
    "    return (padded_sample, attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_map = {0: \"Squats\", 1: \"Deadlifts\", 2: \"Shoulder Press\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final = HierarchicalTransformer(\n",
    "   num_joints=33,\n",
    "    num_frames=201,\n",
    "    d_model=64,\n",
    "    nhead=2,\n",
    "    num_spatial_layers=1,\n",
    "    num_temporal_layers=1,\n",
    "    num_classes=3,\n",
    "    dim_feedforward=2048,\n",
    "    dropout=0.1\n",
    ")\n",
    "model_final.load_state_dict(torch.load(\"../../models/hierarchical_transformer/hierarchical_transformer_f201_d64_h2_s1_t1_do0.1_20250630_0325.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model_inference(video_to_test_path):\n",
    "    x_np, mask_np = load_and_extract_for_inference(video_to_test_path, max_frames=200)\n",
    "    \n",
    "    x_np = x_np[:, :, :3] # get x y z only\n",
    "    x_tensor = torch.from_numpy(x_np).float().unsqueeze(0) # Add batch dimension\n",
    "    mask_tensor = torch.from_numpy(mask_np).float().unsqueeze(0) # Add batch dimension for mask\n",
    "\n",
    "    # Ensure tensors are on the correct device (CPU or GPU)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model_final.to(device)\n",
    "    x_tensor = x_tensor.to(device)\n",
    "    mask_tensor = mask_tensor.to(device)\n",
    "\n",
    "    with torch.no_grad(): # Disable gradient calculation for inference\n",
    "        # Pass both the input data and the attention mask to the model\n",
    "        logits = model_final(x_tensor, temporal_mask=mask_tensor)\n",
    "\n",
    "        # Get the predicted class (highest logit)\n",
    "        predicted_class_idx = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "        # Get the probabilities (if needed)\n",
    "        probabilities = torch.softmax(logits, dim=1)[0] # [0] because it's a batch of 1\n",
    "\n",
    "    # Print results\n",
    "    print(\"New Model (V2):\")\n",
    "    print(f\"\\nVideo: {video_to_test_path}\")\n",
    "    print(f\"Predicted Class Index: {predicted_class_idx}\")\n",
    "    print(f\"Predicted Exercise: {labels_map.get(predicted_class_idx, 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------\n",
      "Processing ../../data/unseen/squat_neil_2.mp4: 534x720, 39 frames\n",
      "Extracted and normalized 39 frames from ../../data/unseen/squat_neil_2.mp4\n",
      "New Model (V2):\n",
      "\n",
      "Video: ../../data/unseen/squat_neil_2.mp4\n",
      "Predicted Class Index: 2\n",
      "Predicted Exercise: Shoulder Press\n"
     ]
    }
   ],
   "source": [
    "\n",
    "video_to_test_path = '../../data/unseen/squat_neil_2.mp4'\n",
    "print(\"--------------------------------------------------------------------------------------\")\n",
    "final_model_inference(video_to_test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
